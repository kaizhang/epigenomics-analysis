[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A guide to the single-cell epigenomics analysis",
    "section": "",
    "text": "Preface\nThis book is used to complement the documentation of the SnapATAC2 Python/Rust package."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  What is epigenomics?",
    "section": "",
    "text": "(Work in progress)"
  },
  {
    "objectID": "anndata.html#introduction",
    "href": "anndata.html#introduction",
    "title": "2  AnnData – Annotated Data",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nAnnData is both a data structure and an on-disk file specification that facilitates the sharing of labeled data matrices.\nThe Python anndata package supports both in-memory and on-disk representation of AnnData object. For detailed descriptions about the AnnData format, please read anndata’s documentation.\nDespite being an excellent package, the anndata package falls short of its support for the on-disk representation or backed mode of AnnData object. When opened in the backed mode, the in-memory snapshot and on-disk data of AnnData are not in sync with each other, causing inconsistent and unexpected behaviors. For example in the backed mode, anndata only supports updates to the X slot in the AnnData object, which means any changes to other slots like obs will not be written to disk. This make the backed mode very cumbersome to use and often lead to unexpected outcomes. Also, as it still reads all other componenets except X into memory, it uses a lot of memory for large datasets.\nTo address these limitations, SnapATAC2 implements its own out-of-core AnnData object with the following key features:\n\nAnnData is fully backed by the underlying hdf5 file. Any operations on the AnnData object will be reflected on the hdf5 file.\nAll elements are lazily loaded. No matter how large is the file, opening it consume almost zero memory. Matrix data can be accessed and processed by chunks, which keeps the memory usage to the minimum.\nIn-memory cache can be turned on to speed up the repetitive access of elements.\nFeaturing an AnnDataSet object to lazily concatenate multiple AnnData objects."
  },
  {
    "objectID": "anndata.html#a-tutorial-on-using-backed-anndata-objects",
    "href": "anndata.html#a-tutorial-on-using-backed-anndata-objects",
    "title": "2  AnnData – Annotated Data",
    "section": "2.2 A tutorial on using backed AnnData objects",
    "text": "2.2 A tutorial on using backed AnnData objects\nIn this section, we will learn the basics about SnapATAC2’s AnnData implementation.\n\n2.2.1 Reading/opening a h5ad file.\nSnapATAC2 can open h5ad files in either in-memory mode or backed mode. By default, snapatac2.read open a h5ad file in backed mode.\n\nimport snapatac2 as snap\nadata = snap.read(snap.datasets.pbmc5k(type='h5ad'))\nadata\n\nAnnData object with n_obs x n_vars = 4363 x 6176550 backed at '/home/kaizhang/.cache/snapatac2/atac_pbmc_5k.h5ad'\n    obs: 'tsse', 'n_fragment', 'frac_dup', 'frac_mito', 'doublet_score', 'is_doublet', 'leiden'\n    var: 'selected'\n    uns: 'scrublet_threshold', 'reference_sequences', 'scrublet_sim_doublet_score', 'spectral_eigenvalue'\n    obsm: 'X_umap', 'X_spectral', 'insertion'\n    obsp: 'distances'\n\n\nYou can turn the backed mode off using backed=False, which will use the Python anndata package to read the file and create an in-memory AnnData object.\n\nimport snapatac2 as snap\nadata = snap.read(snap.datasets.pbmc5k(type='h5ad'), backed=None)\nadata\n\nUpdating file 'atac_pbmc_5k.h5ad' from 'https://data.mendeley.com/api/datasets/dr2z4jbcx3/draft/files/d90adfd1-b4b8-4dcd-8704-9ab19f104116?a=758c37e5-4832-4c91-af89-9a1a83a051b3' to '/home/kaizhang/.cache/snapatac2'.\n\n\nAnnData object with n_obs × n_vars = 4363 × 6176550\n    obs: 'tsse', 'n_fragment', 'frac_dup', 'frac_mito', 'doublet_score', 'is_doublet', 'leiden'\n    var: 'selected'\n    uns: 'reference_sequences', 'scrublet_sim_doublet_score', 'scrublet_threshold', 'spectral_eigenvalue'\n    obsm: 'X_spectral', 'X_umap', 'insertion'\n    obsp: 'distances'\n\n\n\n\n2.2.2 Closing a backed AnnData object\nThe backed AnnData object in SnapATAC2 does not need to be saved as it is always in sync with the data on disk. However, if you have opened the h5ad file in write mode, it is important to remember to close the file using the AnnData.close method. Otherwise, the underlying hdf5 file might be corrupted.\n\nadata = snap.read(snap.datasets.pbmc5k(type='h5ad'))\nadata.close()\nadata\n\nClosed AnnData object\n\n\n\n\n2.2.3 Creating a backed AnnData object\nYou can use the AnnData constructor to create a new AnnData object.\n\nadata = snap.AnnData(filename='adata.h5ad')\nadata\n\nAnnData object with n_obs x n_vars = 0 x 0 backed at 'adata.h5ad'\n\n\nYou can then modify slots in the AnnData object.\n\nimport numpy as np\nadata.X = np.ones((3, 4))\nadata.obs_names = [\"1\", \"2\", \"3\"]\nadata.var_names = [\"a\", \"b\", \"c\", \"d\"]\nadata.obsm['matrix'] = np.ones((3, 10))\nadata.varm['another_matrix'] = np.ones((4, 10))\nadata\n\nAnnData object with n_obs x n_vars = 3 x 4 backed at 'adata.h5ad'\n    obsm: 'matrix'\n    varm: 'another_matrix'\n\n\nThe matrices are now saved on the backing hdf5 file and will be cleared from the memory.\n\n\n2.2.4 Accessing elements in a backed AnnData object\nSlots in backed AnnData object, e.g., AnnData.X, AnnData.obs, store references to the actual data. Accessing those slots does not automatically perform dereferencing or load the data into memory. Instead, a lazy element will be returned, as demonstrated in the example below:\n\nadata.X\n\nArray(f64) element, cache_enabled: no, cached: no\n\n\nHowever, asscessing the slots by keys will automatically read the data:\n\nadata.obsm['matrix']\n\narray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n\n\nTo retreive the lazy element from obsm, you can use:\n\nadata.obsm.el('matrix')\n\nArray(f64) element, cache_enabled: no, cached: no\n\n\nSeveral useful methods haven been implemented for lazy elements. For example, you can use the slicing operator to read the full data or a part of the data:\n\nadata.X[:]\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\n\nadata.X[:2, :2]\n\narray([[1., 1.],\n       [1., 1.]])\n\n\nYou can also iterate over the chunks of the matrix using the chunked method:\n\nfor chunk, fr, to in adata.obsm.el('matrix').chunked(chunk_size=2):\n    print(\"from row {} to {}: {}\".format(fr, to - 1, chunk))\n\nfrom row 0 to 1: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\nfrom row 2 to 2: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n\nBy default AnnData will read from the disk each time you request the data. This will incur a lot of IO overheads if you do this repetitively.\n\n%%time\nfor _ in range(1000):\n    adata.obsm['matrix']\n\nCPU times: user 69.3 ms, sys: 1.78 ms, total: 71.1 ms\nWall time: 70.9 ms\n\n\nOne solution to this is to turn on the cache for the element you want to repetitively read from.\n\n%%time\nadata.obsm.el('matrix').enable_cache()\nfor _ in range(1000):\n    adata.obsm['matrix']\n\nCPU times: user 1.94 ms, sys: 3 µs, total: 1.95 ms\nWall time: 1.95 ms\n\n\nThe data will be cached the first time you request it and the subsequent calls will make use of the cached data.\n\n\n2.2.5 Subsetting the AnnData\nThe backed AnnData object does not have “views”. Instead, you need to use the AnnData.subset method to create a new AnnData object.\n\nadata_subset = adata.subset([0, 1], [0, 1], out=\"subset.h5ad\")\nadata_subset\n\nAnnData object with n_obs x n_vars = 2 x 2 backed at 'subset.h5ad'\n    obsm: 'matrix'\n    varm: 'another_matrix'\n\n\nYou could also do this inplace without the out parameter:\n\nadata_subset.subset([0])\nadata_subset\n\nAnnData object with n_obs x n_vars = 1 x 2 backed at 'subset.h5ad'\n    obsm: 'matrix'\n    varm: 'another_matrix'\n\n\n\n\n2.2.6 Convert to in-memory representation\nFinally, you can convert a backed AnnData to anndata’s in-memory AnnData object using:\n\nadata.to_memory()\n\nAnnData object with n_obs × n_vars = 3 × 4\n    obsm: 'matrix'\n    varm: 'another_matrix'"
  },
  {
    "objectID": "anndata.html#combining-multiple-anndata-objects-into-a-anndataset-object",
    "href": "anndata.html#combining-multiple-anndata-objects-into-a-anndataset-object",
    "title": "2  AnnData – Annotated Data",
    "section": "2.3 Combining multiple AnnData objects into a AnnDataSet object",
    "text": "2.3 Combining multiple AnnData objects into a AnnDataSet object\nOftentimes you want to combine and deal with multiple h5ad files simultaniously. In this section you will learn how to do this efficiently.\nFirst, let us create a bunch of AnnData objects.\n\ndef create_anndata(index: int):\n    adata = snap.AnnData(\n        X=np.ones((4, 7))*index,\n        filename=str(index) + \".h5ad\",\n    )\n    adata.var_names = [str(i) for i in range(7)]\n    adata.obs_names = [str(i) for i in range(4)]\n    adata.obsm['matrix'] = np.random.rand(4,50)\n    return adata\nlist_of_anndata = [(str(i), create_anndata(i)) for i in range(10)]\n\nWe can then use the AnnDataSet constructor to horizontally concatenate all AnnData objects.\n\ndataset = snap.AnnDataSet(\n    adatas=list_of_anndata,\n    filename=\"dataset.h5ads\",\n    add_key=\"id\",\n)\ndataset\n\nAnnDataSet object with n_obs x n_vars = 40 x 7 backed at 'dataset.h5ads'\ncontains 10 AnnData objects with keys: '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'\n    obs: 'id'\n    uns: 'AnnDataSet'\n\n\nAnnDataSet is just a special form of AnnData objects. It inherits most of the methods from AnnData. It carries its own annotations, such as obs, var, obsm, etc. Besides, it grants you the access to component AnnData objects as well, as shown in the example below:\n\ndataset.adatas.obsm['matrix']\n\narray([[0.58093577, 0.26644723, 0.4864984 , ..., 0.71743106, 0.9752404 ,\n        0.14249661],\n       [0.84127877, 0.70899659, 0.70174468, ..., 0.55876489, 0.71954284,\n        0.07213879],\n       [0.59091501, 0.69258951, 0.49688508, ..., 0.67908653, 0.62798061,\n        0.79639072],\n       ...,\n       [0.34293689, 0.60989453, 0.24111227, ..., 0.21526474, 0.0989522 ,\n        0.57523261],\n       [0.56398127, 0.2169606 , 0.998914  , ..., 0.02527175, 0.31128978,\n        0.37318406],\n       [0.11068251, 0.28360159, 0.51183271, ..., 0.40982264, 0.03265828,\n        0.90912685]])\n\n\n\n2.3.1 Subsetting an AnnDataSet object\nAnnDataSet can be subsetted in a way similar to AnnData objects. But there is one caveat: subsetting an AnnDataSet will not rearrange the rows across component AnnData objects.\n\n\n2.3.2 Converting AnnDataSet to AnnData\nAn in-memory AnnData can be made from AnnDataSet using:\n\ndataset.to_adata()\n\nAnnData object with n_obs × n_vars = 40 × 7\n    obs: 'id'\n    uns: 'AnnDataSet'"
  },
  {
    "objectID": "file_format.html#what-is-atac-seq",
    "href": "file_format.html#what-is-atac-seq",
    "title": "3  Input data format",
    "section": "3.1 What is ATAC-seq?",
    "text": "3.1 What is ATAC-seq?\nATAC-Seq stands for Assay for Transposase-Accessible Chromatin with high-throughput sequencing."
  },
  {
    "objectID": "file_format.html#single-cell-atac-seq-scatac-seq",
    "href": "file_format.html#single-cell-atac-seq-scatac-seq",
    "title": "3  Input data format",
    "section": "3.2 Single-Cell ATAC-Seq (scATAC-Seq)",
    "text": "3.2 Single-Cell ATAC-Seq (scATAC-Seq)\nA major limitation of all methods that use populations of millions of cells is that the data they produce is always an average of what’s happening in each of the individual cells in the population, which averages out the cell-to-cell variability that might be present in the sample and therefore might eliminate the ability to make observations about interesting phenomena in sub-populations. This variability is often an important feature of biology when it comes to things like tumor heterogeneity or developmental processes.\nSnapATAC2 accepts BAM or BED-like tabular file as input. The BED-like tabular file can be used to represent fragments (paired-end sequencing) or insertions (single-end sequencing). BAM files can be converted to BED-like files using snapatac2.pp.make_fragment_file."
  },
  {
    "objectID": "file_format.html#fragment-interval-format",
    "href": "file_format.html#fragment-interval-format",
    "title": "3  Input data format",
    "section": "3.3 Fragment interval format",
    "text": "3.3 Fragment interval format\nFragments are created by two separate transposition events, which create the two ends of the observed fragment. Each unique fragment may generate multiple duplicate reads. These duplicate reads are collapsed into a single fragment record. A fragment record must contain exactly five fields:\n\nReference genome chromosome of fragment.\nAdjusted start position of fragment on chromosome.\nAdjusted end position of fragment on chromosome. The end position is exclusive, so represents the position immediately following the fragment interval.\nThe cell barcode of this fragment.\nThe total number of read pairs associated with this fragment. This includes the read pair marked unique and all duplicate read pairs.\n\nDuring data import, a fragment record is converted to two insertions corresponding to the start and end position of the fragment interval."
  },
  {
    "objectID": "file_format.html#insertion-format",
    "href": "file_format.html#insertion-format",
    "title": "3  Input data format",
    "section": "3.4 Insertion format",
    "text": "3.4 Insertion format\nInsertion records are used to represent single-end reads in experiments that sequence only one end of the fragments, e.g., Paired-Tag experiments. While fragment records are created by two transposition events, insertion records correspond to a single transposition event.\nEach insertion record must contain six fields:\n\nReference genome chromosome.\nAdjusted start position on chromosome.\nAdjusted end position on chromosome. The end position is exclusive.\nThe cell barcode of this fragment.\nThe total number of reads associated with this insertion.\nThe strandness of the read.\n\nDuring data import, the 5’ end of an insertion record is converted to one insertion count.\nNote: in both cases, the fifth column (duplication count) is not used during reads counting. In other words, we count duplicated reads only once. If you want to count the same record multiple times, you need to duplicate them in the input file."
  },
  {
    "objectID": "dim_reduct.html#preprocessing",
    "href": "dim_reduct.html#preprocessing",
    "title": "4  Dimension reduction",
    "section": "4.1 Preprocessing",
    "text": "4.1 Preprocessing\nWe preprocess the matrix by the Inverse Document Frequency (IDF) weighting. In the context of scATAC-seq, the IDF is defined as:"
  },
  {
    "objectID": "dim_reduct.html#spectral-embedding",
    "href": "dim_reduct.html#spectral-embedding",
    "title": "4  Dimension reduction",
    "section": "4.2 Spectral embedding",
    "text": "4.2 Spectral embedding\nAssuming the \\(n \\times p\\) cell by feature count matrix \\(C\\) has been preprocessed, we first compute the \\(n \\times n\\) pairwise similarity matrix \\(W\\) such that \\(W_{ij} = \\delta(C_{i*}, C_{j*})\\), where \\(\\delta: \\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}\\) is the function defines the similarity between any two cells. Typical choices of \\(\\delta\\) include the jaccard index and the cosine similarity.\nWe then compute the symmetric normalized graph Laplacian \\(L_{sym} = I - D^{-1/2} W D^{-1/2}\\), where \\(I\\) is the identity matrix and \\(D = diag(W1)\\).\nThe bottom eigenvectors of \\(L_{sym}\\) are selected as the lower dimensional embedding. The corresponding eigenvectors can be computed alternatively as the top eigenvectors of the similarly normalized weight matrix:\n\\(\\tilde{W} = D^{-1/2} W D^{-1/2}\\),"
  },
  {
    "objectID": "dim_reduct.html#matrix-free-spectral-embedding-with-cosine-similarity",
    "href": "dim_reduct.html#matrix-free-spectral-embedding-with-cosine-similarity",
    "title": "4  Dimension reduction",
    "section": "4.3 Matrix-free spectral embedding with cosine similarity",
    "text": "4.3 Matrix-free spectral embedding with cosine similarity\nWhen using the cosine similarity, we can avoid computing the full similarity matrix.\nThe cosine similarity between two vectors A and B is defined as:\n\\[S_c(A, B) = \\frac{A \\cdot B}{||A|| ||B||}\\]\nFirst we rescale the non-negative count matrix \\(C\\) to \\(X\\) such that the rows of \\(X\\) have unit \\(L_2\\) norm.\nThe cosine similarity matrix is then defined as,\n\\[W = XX^T - I\\]\nNote that we set the diagonal of \\(W\\) to zeros by subtracting the identity matrix. This is necessary because our benchmark result show that it generally improves the quality of the embedding.\nThe degree matrix can be computed as,\n\\[D = diag((X X^T - I) \\mathbf{1}) = diag(X(X^T \\mathbf{1}) - \\mathbf{1})\\]\nand,\n\\[\\tilde{W} = D^{-1/2} XX^T D^{-1/2} - D^{-1} = \\tilde{X}\\tilde{X}^T - D^{-1}\\]\nwhere \\(\\tilde{X} = D^{-1/2} X\\).\nNote that \\(\\tilde{X}\\) has the same size as \\(X\\), and if X is sparse, \\(\\tilde{X}\\) preserves the sparsity pattern of \\(X\\).\nWe remark that this problem would be easier if we ignore the \\(D^{-1}\\) term, because the eigenvectors of \\(\\tilde{X}\\tilde{X}^T\\) can be computed from the Singular Vector Decomposition (SVD) of \\(\\tilde{X}\\). With the presence of the \\(D^{-1}\\) term, we resort to the Lanczos algorithm to compute the top eigenvectors of \\(\\tilde{W}\\) without ever computing \\(\\tilde{W}\\). Each iteration in the Lanczos algorithm requires computing the matrix-vector product between \\(\\tilde{W}\\) and \\(\\mathbf{v}\\),\n\\[\\tilde{W} \\mathbf{v} = \\tilde{X} (\\tilde{X}^T \\mathbf{v}) - D^{-1} \\mathbf{v}\\]\nUsing the specific order of operations shown in the formula above, we can reduce the computational cost of the matrix-vector product to \\(2z + n\\), where \\(n\\) is the number of rows in \\(X\\) and \\(z\\) is the number of non-zero elements in \\(X\\).\nAs a comparision, performing this operation on the full similarity matrix will need \\(n^2\\) computations. Note that \\(z ≪ n^2\\) for most scATAC-seq data. Computing the full similarity matrix additionally requires \\(n^3\\) computations using the naive algorithm, which is prohibitively expensive for large datasets. Therefore, the matrix-free method is much faster and more memory efficient."
  },
  {
    "objectID": "dim_reduct.html#nyström-method",
    "href": "dim_reduct.html#nyström-method",
    "title": "4  Dimension reduction",
    "section": "4.4 Nyström method",
    "text": "4.4 Nyström method\nThe matrix-free method described above is very fast and memory efficient. However, for massive datasets with hundreds of millions of cells, storing the cell by peak count matrix \\(C\\) may already be a challenge. In this section, we describe an on-line embedding method that can applied to virtually arbitrary large datasets. The key idea here is to use the Nystrom method to perform a low-rank approximation of the full similarity matrix.\nWe will be focusing on generating an approximation \\(\\tilde{W}\\) of \\(W\\) based on a sample of \\(l ≪ n\\) of its columns.\nSuppose \\(W = \\begin{bmatrix} A & B \\\\ B^T & C \\end{bmatrix}\\) and columns \\(\\begin{bmatrix} A \\\\ B^T \\end{bmatrix}\\) are our samples. We first perform eigendecomposition on \\(A = U \\Lambda U^T\\). The nystrom method approximates the eigenvectors of matrix \\(W\\) by \\(\\tilde{U} = \\begin{bmatrix} U \\\\ B^T U \\Lambda^{-1} \\end{bmatrix}\\).\nWe can then compute \\(\\tilde{W}\\):\n\\[\n\\begin{aligned}\n\\tilde{W} &= \\tilde{U} \\Lambda \\tilde{U}^T \\\\\n          &= \\begin{bmatrix} U \\\\ B^T U \\Lambda^{-1} \\end{bmatrix}\n             \\Lambda\n             \\begin{bmatrix} U^T & \\Lambda^{-1}U^TB \\end{bmatrix} \\\\\n          &= \\begin{bmatrix}\n               U \\Lambda U^T & U \\Lambda \\Lambda^{-1} U^T B \\\\\n               B^T U \\Lambda^{-1} \\Lambda U^T & B^T U \\Lambda^{-1} \\Lambda \\Lambda^{-1} U^T B\n             \\end{bmatrix} \\\\\n          &= \\begin{bmatrix} A & B \\\\ B^T & B^T U \\Lambda^{-1} U^T B \\end{bmatrix}\n\\end{aligned}\n\\]\nIn practice, \\(\\tilde{W}\\) does not need to be computed. Instead, it is used implicitly to estimate the degree normalization vector:\n\\[\n\\tilde{d} = \\tilde{W}\\mathbf{1} = \\begin{bmatrix}A\\mathbf{1} + B\\mathbf{1} \\\\ B^T \\mathbf{1} + B^T A^{-1} B\\mathbf{1}\n\\end{bmatrix}\n\\]\nThis approach requires computing the inverse of \\(A\\), which is expensive when \\(A\\) is large. Here we use an algorithm reported in XXX to approximate the degree matrix.\n\n\n\n\nFang, Rongxin, Sebastian Preissl, Yang Li, Xiaomeng Hou, Jacinta Lucero, Xinxin Wang, Amir Motamedi, et al. 2021. “Comprehensive analysis of single cell ATAC-seq data with SnapATAC.” Nature Communications 12 (1): 1337. https://doi.org/10.1038/s41467-021-21583-9."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Fang, Rongxin, Sebastian Preissl, Yang Li, Xiaomeng Hou, Jacinta Lucero,\nXinxin Wang, Amir Motamedi, et al. 2021. “Comprehensive analysis of single cell ATAC-seq data with\nSnapATAC.” Nature Communications 12 (1): 1337. https://doi.org/10.1038/s41467-021-21583-9."
  }
]